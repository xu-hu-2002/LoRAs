# å…±äº«è®­ç»ƒå·¥å…·å‡½æ•°
# æä¾›ç»Ÿä¸€çš„è®­ç»ƒæ¥å£å’Œå·¥å…·

import torch
from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling, AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel
from datasets import Dataset
from typing import Optional, Dict, Any
import os
from datetime import datetime

def create_training_arguments(
    output_dir: str = "./results",
    num_train_epochs: int = 3,
    per_device_train_batch_size: int = 4,
    gradient_accumulation_steps: int = 2,
    learning_rate: float = 2e-4,
    warmup_steps: int = 100,
    logging_steps: int = 10,
    save_steps: int = 500,
    eval_steps: Optional[int] = None,
    evaluation_strategy: str = "no",
    save_strategy: str = "steps",
    fp16: bool = True,
    bf16: bool = False,
    max_grad_norm: float = 1.0,
    dataloader_num_workers: int = 0,
    remove_unused_columns: bool = False,
    **kwargs
) -> TrainingArguments:
    """
    åˆ›å»ºè®­ç»ƒå‚æ•°
    
    Args:
        output_dir: è¾“å‡ºç›®å½•
        num_train_epochs: è®­ç»ƒè½®æ•°
        per_device_train_batch_size: æ¯è®¾å¤‡batch size
        gradient_accumulation_steps: æ¢¯åº¦ç´¯ç§¯æ­¥æ•°
        learning_rate: å­¦ä¹ ç‡ï¼ˆPEFTæ¨è2e-4ï¼‰
        warmup_steps: é¢„çƒ­æ­¥æ•°
        logging_steps: æ—¥å¿—è®°å½•æ­¥æ•°
        save_steps: æ¨¡å‹ä¿å­˜æ­¥æ•°
        eval_steps: è¯„ä¼°æ­¥æ•°
        evaluation_strategy: è¯„ä¼°ç­–ç•¥
        save_strategy: ä¿å­˜ç­–ç•¥
        fp16: æ˜¯å¦ä½¿ç”¨FP16
        bf16: æ˜¯å¦ä½¿ç”¨BF16
        max_grad_norm: æ¢¯åº¦è£å‰ª
        dataloader_num_workers: æ•°æ®åŠ è½½å·¥ä½œè¿›ç¨‹æ•°
        remove_unused_columns: æ˜¯å¦ç§»é™¤æœªä½¿ç”¨çš„åˆ—
        **kwargs: å…¶ä»–å‚æ•°
    
    Returns:
        TrainingArguments: è®­ç»ƒå‚æ•°
    """
    
    # åˆ›å»ºå¸¦æ—¶é—´æˆ³çš„è¾“å‡ºç›®å½•
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_dir = f"{output_dir}_{timestamp}"
    
    training_args = TrainingArguments(
        output_dir=output_dir,
        num_train_epochs=num_train_epochs,
        per_device_train_batch_size=per_device_train_batch_size,
        gradient_accumulation_steps=gradient_accumulation_steps,
        learning_rate=learning_rate,
        warmup_steps=warmup_steps,
        logging_steps=logging_steps,
        save_steps=save_steps,
        eval_steps=eval_steps,
        evaluation_strategy=evaluation_strategy,
        save_strategy=save_strategy,
        fp16=fp16,
        bf16=bf16,
        max_grad_norm=max_grad_norm,
        dataloader_num_workers=dataloader_num_workers,
        remove_unused_columns=remove_unused_columns,
        report_to=[],  # ç¦ç”¨wandbç­‰æŠ¥å‘Š
        save_total_limit=2,  # åªä¿ç•™æœ€è¿‘2ä¸ªcheckpoint
        load_best_model_at_end=False,
        **kwargs
    )
    
    print(f"è®­ç»ƒå‚æ•°:")
    print(f"  è¾“å‡ºç›®å½•: {output_dir}")
    print(f"  è®­ç»ƒè½®æ•°: {num_train_epochs}")
    print(f"  Batch size: {per_device_train_batch_size}")
    print(f"  æ¢¯åº¦ç´¯ç§¯: {gradient_accumulation_steps}")
    print(f"  å­¦ä¹ ç‡: {learning_rate}")
    print(f"  æ··åˆç²¾åº¦: FP16={fp16}, BF16={bf16}")
    
    return training_args

def create_trainer(
    model,
    tokenizer,
    train_dataset: Dataset,
    eval_dataset: Optional[Dataset] = None,
    training_args: Optional[TrainingArguments] = None,
    data_collator = None
) -> Trainer:
    """
    åˆ›å»ºè®­ç»ƒå™¨
    
    Args:
        model: æ¨¡å‹
        tokenizer: tokenizer
        train_dataset: è®­ç»ƒæ•°æ®é›†
        eval_dataset: éªŒè¯æ•°æ®é›†
        training_args: è®­ç»ƒå‚æ•°
        data_collator: æ•°æ®æ•´ç†å™¨
    
    Returns:
        Trainer: è®­ç»ƒå™¨
    """
    
    if training_args is None:
        training_args = create_training_arguments()
    
    if data_collator is None:
        data_collator = DataCollatorForLanguageModeling(
            tokenizer=tokenizer,
            mlm=False  # å¯¹äºcausal LMï¼Œä¸ä½¿ç”¨MLM
        )
    
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
        tokenizer=tokenizer,
        data_collator=data_collator,
    )
    
    print(f"è®­ç»ƒå™¨åˆ›å»ºå®Œæˆ")
    print(f"  è®­ç»ƒæ ·æœ¬æ•°: {len(train_dataset)}")
    if eval_dataset:
        print(f"  éªŒè¯æ ·æœ¬æ•°: {len(eval_dataset)}")
    
    return trainer

def train_model(
    model,
    tokenizer,
    train_dataset: Dataset,
    eval_dataset: Optional[Dataset] = None,
    training_args: Optional[TrainingArguments] = None,
    resume_from_checkpoint: Optional[str] = None
) -> Trainer:
    """
    è®­ç»ƒæ¨¡å‹çš„å®Œæ•´æµç¨‹
    
    Args:
        model: æ¨¡å‹
        tokenizer: tokenizer
        train_dataset: è®­ç»ƒæ•°æ®é›†
        eval_dataset: éªŒè¯æ•°æ®é›†
        training_args: è®­ç»ƒå‚æ•°
        resume_from_checkpoint: ä»checkpointæ¢å¤è®­ç»ƒ
    
    Returns:
        Trainer: è®­ç»ƒå®Œæˆçš„è®­ç»ƒå™¨
    """
    
    print("="*60)
    print("å¼€å§‹æ¨¡å‹è®­ç»ƒ")
    print("="*60)
    
    # æ‰“å°æ¨¡å‹ä¿¡æ¯
    if hasattr(model, 'print_trainable_parameters'):
        model.print_trainable_parameters()
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = create_trainer(
        model=model,
        tokenizer=tokenizer,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
        training_args=training_args
    )
    
    # å¼€å§‹è®­ç»ƒ
    try:
        trainer.train(resume_from_checkpoint=resume_from_checkpoint)
        print("è®­ç»ƒå®Œæˆï¼")
    except Exception as e:
        print(f"è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        raise
    
    return trainer

def save_model_and_adapter(
    model,
    tokenizer,
    save_dir: str,
    save_adapter_only: bool = True
):
    """
    ä¿å­˜æ¨¡å‹å’Œé€‚é…å™¨
    
    Args:
        model: æ¨¡å‹
        tokenizer: tokenizer
        save_dir: ä¿å­˜ç›®å½•
        save_adapter_only: æ˜¯å¦åªä¿å­˜é€‚é…å™¨
    """
    
    print(f"ä¿å­˜æ¨¡å‹åˆ°: {save_dir}")
    
    # åˆ›å»ºä¿å­˜ç›®å½•
    os.makedirs(save_dir, exist_ok=True)
    
    # ä¿å­˜tokenizer
    tokenizer.save_pretrained(save_dir)
    print("Tokenizerå·²ä¿å­˜")
    
    if save_adapter_only and isinstance(model, PeftModel):
        # åªä¿å­˜PEFTé€‚é…å™¨
        model.save_pretrained(save_dir)
        print("PEFTé€‚é…å™¨å·²ä¿å­˜")
    else:
        # ä¿å­˜å®Œæ•´æ¨¡å‹
        model.save_pretrained(save_dir)
        print("å®Œæ•´æ¨¡å‹å·²ä¿å­˜")

def merge_and_save_model(
    model,
    tokenizer,
    save_dir: str,
    save_merged_model: bool = True,
    save_adapter_separately: bool = False
):
    """
    åˆå¹¶adapteræƒé‡åˆ°base modelå¹¶ä¿å­˜å®Œæ•´æ¨¡å‹
    
    Args:
        model: PEFTæ¨¡å‹
        tokenizer: tokenizer
        save_dir: ä¿å­˜ç›®å½•
        save_merged_model: æ˜¯å¦ä¿å­˜åˆå¹¶åçš„å®Œæ•´æ¨¡å‹
        save_adapter_separately: æ˜¯å¦åŒæ—¶ä¿å­˜adapterå‰¯æœ¬
    
    Returns:
        merged_model: åˆå¹¶åçš„æ¨¡å‹ï¼ˆå¦‚æœè¿›è¡Œäº†åˆå¹¶ï¼‰
    """
    from peft import PeftModel
    
    print(f"ğŸ”„ å¼€å§‹æ¨¡å‹åˆå¹¶æµç¨‹...")
    
    if isinstance(model, PeftModel):
        print(f"âœ… æ£€æµ‹åˆ°PEFTæ¨¡å‹ï¼Œå¼€å§‹åˆå¹¶adapteræƒé‡...")
        
        # è·å–æ¨¡å‹å¤§å°ä¿¡æ¯ï¼ˆåˆå¹¶å‰ï¼‰
        original_size = get_model_size(model)
        print(f"ğŸ“Š åŸå§‹PEFTæ¨¡å‹å¤§å°: {original_size:.2f}MB")
        
        # åˆå¹¶adapteræƒé‡åˆ°base model
        merged_model = model.merge_and_unload()
        
        # è·å–åˆå¹¶åæ¨¡å‹å¤§å°
        merged_size = get_model_size(merged_model)
        print(f"ğŸ“Š åˆå¹¶åæ¨¡å‹å¤§å°: {merged_size:.2f}MB")
        
        if save_merged_model:
            # åˆ›å»ºä¿å­˜ç›®å½•
            os.makedirs(save_dir, exist_ok=True)
            
            # ä¿å­˜å®Œæ•´çš„åˆå¹¶æ¨¡å‹
            merged_model.save_pretrained(save_dir)
            tokenizer.save_pretrained(save_dir)
            print(f"âœ… åˆå¹¶æ¨¡å‹å·²ä¿å­˜åˆ°: {save_dir}")
            
            # ä¿å­˜æ¨¡å‹ä¿¡æ¯æ–‡ä»¶
            info_path = os.path.join(save_dir, "model_info.txt")
            with open(info_path, "w", encoding="utf-8") as f:
                f.write("# Model Merging Information\n")
                f.write(f"Original PEFT model size: {original_size:.2f}MB\n")
                f.write(f"Merged model size: {merged_size:.2f}MB\n")
                f.write(f"Merge timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                f.write(f"PEFT config: {model.peft_config}\n")
            print(f"ğŸ“ æ¨¡å‹ä¿¡æ¯å·²ä¿å­˜åˆ°: {info_path}")
        
        # å¯é€‰ï¼šåŒæ—¶ä¿å­˜adapterå‰¯æœ¬
        if save_adapter_separately:
            adapter_dir = f"{save_dir}_adapter"
            os.makedirs(adapter_dir, exist_ok=True)
            model.save_pretrained(adapter_dir)
            tokenizer.save_pretrained(adapter_dir)
            print(f"ğŸ’¾ Adapterå‰¯æœ¬å·²ä¿å­˜åˆ°: {adapter_dir}")
        
        return merged_model
    else:
        print("âš ï¸ æ¨¡å‹ä¸æ˜¯PEFTæ¨¡å‹ï¼Œæ— éœ€åˆå¹¶")
        if save_merged_model:
            # ç›´æ¥ä¿å­˜åŸæ¨¡å‹
            os.makedirs(save_dir, exist_ok=True)
            model.save_pretrained(save_dir)
            tokenizer.save_pretrained(save_dir)
            print(f"ğŸ’¾ åŸæ¨¡å‹å·²ä¿å­˜åˆ°: {save_dir}")
        return model

def load_merged_model(model_path: str, torch_dtype=torch.bfloat16, device_map="auto"):
    """
    åŠ è½½åˆå¹¶åçš„å®Œæ•´æ¨¡å‹
    
    Args:
        model_path: æ¨¡å‹è·¯å¾„
        torch_dtype: æ•°æ®ç±»å‹
        device_map: è®¾å¤‡æ˜ å°„
    
    Returns:
        tuple: (model, tokenizer)
    """
    print(f"ğŸ”„ åŠ è½½åˆå¹¶æ¨¡å‹: {model_path}")
    
    # åŠ è½½æ¨¡å‹
    model = AutoModelForCausalLM.from_pretrained(
        model_path,
        torch_dtype=torch_dtype,
        device_map=device_map,
        trust_remote_code=True
    )
    
    # åŠ è½½tokenizer
    tokenizer = AutoTokenizer.from_pretrained(model_path)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    # è·å–æ¨¡å‹å¤§å°
    model_size = get_model_size(model)
    print(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼Œå¤§å°: {model_size:.2f}MB")
    
    return model, tokenizer

def get_model_size(model) -> float:
    """
    è·å–æ¨¡å‹å¤§å°ï¼ˆMBï¼‰
    
    Args:
        model: PyTorchæ¨¡å‹
    
    Returns:
        float: æ¨¡å‹å¤§å°ï¼ˆMBï¼‰
    """
    param_size = 0
    buffer_size = 0
    
    for param in model.parameters():
        param_size += param.nelement() * param.element_size()
    
    for buffer in model.buffers():
        buffer_size += buffer.nelement() * buffer.element_size()
    
    size_mb = (param_size + buffer_size) / 1024 / 1024
    return size_mb

def compare_model_sizes(
    original_model, 
    merged_model, 
    adapter_model=None
) -> Dict[str, float]:
    """
    æ¯”è¾ƒä¸åŒæ¨¡å‹çš„å¤§å°
    
    Args:
        original_model: åŸå§‹baseæ¨¡å‹
        merged_model: åˆå¹¶åæ¨¡å‹
        adapter_model: PEFT adapteræ¨¡å‹ï¼ˆå¯é€‰ï¼‰
    
    Returns:
        Dict[str, float]: å¤§å°æ¯”è¾ƒç»“æœ
    """
    sizes = {}
    
    if original_model is not None:
        sizes["original_base_model"] = get_model_size(original_model)
    
    if merged_model is not None:
        sizes["merged_model"] = get_model_size(merged_model)
    
    if adapter_model is not None and isinstance(adapter_model, PeftModel):
        # è®¡ç®—adapterçš„å¤§å°
        total_params = sum(p.numel() for p in adapter_model.parameters())
        trainable_params = sum(p.numel() for p in adapter_model.parameters() if p.requires_grad)
        sizes["total_model_with_adapter"] = get_model_size(adapter_model)
        sizes["adapter_only_estimated"] = (trainable_params * 4) / 1024 / 1024  # å‡è®¾float32
    
    print("ğŸ“Š æ¨¡å‹å¤§å°æ¯”è¾ƒ:")
    for name, size in sizes.items():
        print(f"  {name}: {size:.2f}MB")
    
    return sizes

def validate_merged_model(
    original_peft_model,
    merged_model,
    tokenizer,
    test_prompt: str = "Hello, how are you?",
    max_new_tokens: int = 20
) -> bool:
    """
    éªŒè¯åˆå¹¶åçš„æ¨¡å‹æ˜¯å¦å·¥ä½œæ­£å¸¸
    
    Args:
        original_peft_model: åŸå§‹PEFTæ¨¡å‹
        merged_model: åˆå¹¶åæ¨¡å‹
        tokenizer: tokenizer
        test_prompt: æµ‹è¯•æç¤º
        max_new_tokens: æœ€å¤§ç”Ÿæˆtokenæ•°
    
    Returns:
        bool: éªŒè¯æ˜¯å¦é€šè¿‡
    """
    print("ğŸ” éªŒè¯åˆå¹¶æ¨¡å‹...")
    
    try:
        # æµ‹è¯•åŸå§‹PEFTæ¨¡å‹
        original_output = generate_text(
            original_peft_model, tokenizer, test_prompt, 
            max_new_tokens=max_new_tokens, temperature=0.0, do_sample=False
        )
        
        # æµ‹è¯•åˆå¹¶åæ¨¡å‹
        merged_output = generate_text(
            merged_model, tokenizer, test_prompt,
            max_new_tokens=max_new_tokens, temperature=0.0, do_sample=False
        )
        
        # æ¯”è¾ƒè¾“å‡ºï¼ˆåº”è¯¥ç›¸åŒæˆ–éå¸¸ç›¸ä¼¼ï¼‰
        print(f"åŸå§‹PEFTæ¨¡å‹è¾“å‡º: {original_output}")
        print(f"åˆå¹¶åæ¨¡å‹è¾“å‡º: {merged_output}")
        
        # ç®€å•éªŒè¯ï¼šæ£€æŸ¥è¾“å‡ºæ˜¯å¦ç›¸åŒ
        is_valid = original_output.strip() == merged_output.strip()
        
        if is_valid:
            print("âœ… æ¨¡å‹åˆå¹¶éªŒè¯é€šè¿‡ï¼")
        else:
            print("âš ï¸ æ¨¡å‹åˆå¹¶éªŒè¯ä¸å®Œå…¨åŒ¹é…ï¼Œä½†è¿™å¯èƒ½æ˜¯æ­£å¸¸çš„ï¼ˆç”±äºæ•°å€¼ç²¾åº¦ï¼‰")
            is_valid = True  # è½»å¾®å·®å¼‚å¯ä»¥æ¥å—
        
        return is_valid
        
    except Exception as e:
        print(f"âŒ æ¨¡å‹åˆå¹¶éªŒè¯å¤±è´¥: {e}")
        return False

def evaluate_model(
    model,
    tokenizer,
    eval_dataset: Dataset,
    batch_size: int = 8
) -> Dict[str, float]:
    """
    è¯„ä¼°æ¨¡å‹æ€§èƒ½
    
    Args:
        model: æ¨¡å‹
        tokenizer: tokenizer
        eval_dataset: è¯„ä¼°æ•°æ®é›†
        batch_size: batch size
    
    Returns:
        Dict[str, float]: è¯„ä¼°æŒ‡æ ‡
    """
    
    print("å¼€å§‹æ¨¡å‹è¯„ä¼°...")
    
    model.eval()
    total_loss = 0.0
    num_batches = 0
    
    from torch.utils.data import DataLoader
    
    dataloader = DataLoader(eval_dataset, batch_size=batch_size, shuffle=False)
    
    with torch.no_grad():
        for batch in dataloader:
            # å°†æ•°æ®ç§»åŠ¨åˆ°è®¾å¤‡
            input_ids = batch['input_ids'].to(model.device)
            attention_mask = batch['attention_mask'].to(model.device)
            labels = batch['labels'].to(model.device)
            
            # å‰å‘ä¼ æ’­
            outputs = model(
                input_ids=input_ids,
                attention_mask=attention_mask,
                labels=labels
            )
            
            total_loss += outputs.loss.item()
            num_batches += 1
    
    avg_loss = total_loss / num_batches
    perplexity = torch.exp(torch.tensor(avg_loss)).item()
    
    metrics = {
        "eval_loss": avg_loss,
        "eval_perplexity": perplexity
    }
    
    print(f"è¯„ä¼°å®Œæˆ:")
    print(f"  Loss: {avg_loss:.4f}")
    print(f"  Perplexity: {perplexity:.4f}")
    
    return metrics

def generate_text(
    model,
    tokenizer,
    prompt: str,
    max_new_tokens: int = 50,
    temperature: float = 0.7,
    do_sample: bool = True,
    top_p: float = 0.9,
    top_k: int = 50
) -> str:
    """
    ç”Ÿæˆæ–‡æœ¬
    
    Args:
        model: æ¨¡å‹
        tokenizer: tokenizer
        prompt: è¾“å…¥æç¤º
        max_new_tokens: æœ€å¤§ç”Ÿæˆtokenæ•°
        temperature: æ¸©åº¦å‚æ•°
        do_sample: æ˜¯å¦é‡‡æ ·
        top_p: nucleus samplingå‚æ•°
        top_k: top-k samplingå‚æ•°
    
    Returns:
        str: ç”Ÿæˆçš„æ–‡æœ¬
    """
    
    model.eval()
    
    # tokenizeè¾“å…¥
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    
    # ç”Ÿæˆ
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            temperature=temperature,
            do_sample=do_sample,
            top_p=top_p,
            top_k=top_k,
            pad_token_id=tokenizer.eos_token_id
        )
    
    # è§£ç 
    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
    
    # ç§»é™¤è¾“å…¥éƒ¨åˆ†
    if generated_text.startswith(prompt):
        generated_text = generated_text[len(prompt):].strip()
    
    return generated_text

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    print("="*60)
    print("è®­ç»ƒå·¥å…·å‡½æ•°ç¤ºä¾‹")
    print("="*60)
    
    # åˆ›å»ºè®­ç»ƒå‚æ•°ç¤ºä¾‹
    training_args = create_training_arguments(
        output_dir="./test_output",
        num_train_epochs=1,
        per_device_train_batch_size=2,
        learning_rate=2e-4
    )
    
    print(f"\nè®­ç»ƒå‚æ•°ç¤ºä¾‹: {training_args.output_dir}")
    
    print("\nä½¿ç”¨æ–¹æ³•:")
    print("1. training_args = create_training_arguments()")
    print("2. trainer = train_model(model, tokenizer, train_dataset, training_args=training_args)")
    print("3. save_model_and_adapter(model, tokenizer, './saved_model')")
    print("4. metrics = evaluate_model(model, tokenizer, eval_dataset)")
    print("5. text = generate_text(model, tokenizer, 'Question: What is AI?')") 