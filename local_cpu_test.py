import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer
from datasets import load_dataset, Dataset
import warnings

# ä»é¡¹ç›®æ–‡ä»¶ä¸­å¯¼å…¥å¿…è¦çš„æ¨¡å—
from config import COMMON_CONFIG
from common.data_preprocessing import load_and_preprocess_data
from LoRA.lora_hf import create_lora_config
from DoRA.dora_hf import create_dora_config
from PiSSA.pissa_hf import create_pissa_config
from QLoRA.qlora_hf import create_qlora_config, create_bnb_config  #QLoRA éœ€è¦GPU æš‚æ—¶ä¸æµ‹è¯•
from AdaLoRA.adalora import LoRALayer, RankAllocator,SVDLinear #å¹¶æœªé›†æˆåˆ°PEFT éœ€è¦å•ç‹¬é…ç½®config
from AdaLoRA.adalora_config import AdaLoRAConfig 
from peft import get_peft_model

# --- å…¨å±€æµ‹è¯•é…ç½® ---
TEST_MODEL = "distilgpt2"  # å›åˆ°è½»é‡æ¨¡å‹ï¼Œé€‚åˆå½“å‰å†…å­˜æƒ…å†µ (82M å‚æ•°)
MAX_SAMPLES = 10 # ä½¿ç”¨10ä¸ªæ ·æœ¬è¿›è¡Œå¿«é€Ÿæµ‹è¯•
TRAIN_SAMPLES = 8 # 8ä¸ªç”¨äºè®­ç»ƒ
VAL_SAMPLES = 2 # 2ä¸ªç”¨äºéªŒè¯
BATCH_SIZE = 1
NUM_EPOCHS = 1

# åœ¨å…¨å±€é…ç½®éƒ¨åˆ†æ·»åŠ 
DISTILGPT2_TARGET_MODULES = ["c_attn", "c_proj"]  # DistilGPT2ä¸“ç”¨

def run_peft_test(peft_method_name, get_config_func, use_bnb=False):
    """
    ä¸€ä¸ªé€šç”¨çš„æµ‹è¯•å‡½æ•°ï¼Œç”¨äºæµ‹è¯•åŸºäºHuggingFace PEFTåº“çš„å„ç§æ–¹æ³•ã€‚

    Args:
        peft_method_name (str): PEFTæ–¹æ³•çš„åç§° (ä¾‹å¦‚, "LoRA", "DoRA")ã€‚
        get_config_func (function): ä¸€ä¸ªè¿”å›PEFTé…ç½®å¯¹è±¡çš„å‡½æ•°ã€‚
        use_bnb (bool): æ˜¯å¦ä¸ºQLoRAä½¿ç”¨BitsAndBytesé‡åŒ–ã€‚
    """
    print(f"\n{'='*20} æ­£åœ¨æµ‹è¯•: {peft_method_name} {'='*20}")

    # --- 1. æ£€æŸ¥ç¯å¢ƒå’ŒåŠ è½½æ¨¡å‹ ---
    if use_bnb and not torch.cuda.is_available():
        warnings.warn(f"âš ï¸  è­¦å‘Š: {peft_method_name} éœ€è¦CUDA GPUï¼Œä½†å½“å‰ç¯å¢ƒä¸å¯ç”¨ã€‚æ­£åœ¨è·³è¿‡æ­¤æµ‹è¯•ã€‚")
        print(f"{'='*20} {peft_method_name} æµ‹è¯•è·³è¿‡ {'='*20}\n")
        return

    print(f"åŠ è½½åŸºç¡€æ¨¡å‹: {TEST_MODEL}...")
    model_kwargs = {}
    if use_bnb:
        bnb_config = create_bnb_config()
        model_kwargs['quantization_config'] = bnb_config
    
    # CPUä¼˜åŒ–ï¼šä½¿ç”¨torch_dtype=torch.float32ï¼Œä½å†…å­˜æ¨¡å¼
    model = AutoModelForCausalLM.from_pretrained(
        TEST_MODEL, 
        torch_dtype=torch.float32,  # CPUä¸Šä½¿ç”¨float32
        low_cpu_mem_usage=True,     # ä½å†…å­˜æ¨¡å¼
        **model_kwargs
    )
    tokenizer = AutoTokenizer.from_pretrained(TEST_MODEL)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    # --- 2. åŠ è½½å’Œå‡†å¤‡æ•°æ® ---
    print("åŠ è½½å¹¶é¢„å¤„ç†TriviaQAæ•°æ®é›†...")
    full_dataset = load_and_preprocess_data(
        dataset_name="trivia_qa", 
        model_name=TEST_MODEL, 
        max_samples=MAX_SAMPLES, 
        split='train' 
    )
    
    train_dataset = Dataset.from_dict(full_dataset[:TRAIN_SAMPLES])
    validation_dataset = Dataset.from_dict(full_dataset[TRAIN_SAMPLES:TRAIN_SAMPLES + VAL_SAMPLES])

    # --- 3. åˆ›å»ºPEFTé…ç½®å¹¶åº”ç”¨åˆ°æ¨¡å‹ ---
    print(f"åˆ›å»º {peft_method_name} é…ç½®...")
    # æ ¹æ®æ¨¡å‹é€‰æ‹©target_modules
    if TEST_MODEL == "distilgpt2":
        target_modules = ["c_attn", "c_proj"]
    else:
        target_modules = COMMON_CONFIG['target_modules']
    
    # åˆ›å»ºé…ç½®æ—¶ä¼ å…¥æ­£ç¡®çš„target_modules
    peft_config = get_config_func(r=8, lora_alpha=16, target_modules=target_modules)
    model = get_peft_model(model, peft_config)
    print(f"åº”ç”¨ {peft_method_name} åçš„æ¨¡å‹:")
    model.print_trainable_parameters()

    # --- 4. è®¾ç½®è®­ç»ƒå‚æ•°å¹¶å¼€å§‹è®­ç»ƒ ---
    training_args = TrainingArguments(
        output_dir=f"./results_{peft_method_name.lower()}_test",
        num_train_epochs=NUM_EPOCHS,
        per_device_train_batch_size=BATCH_SIZE,
        per_device_eval_batch_size=BATCH_SIZE,
        logging_dir=f'./logs_{peft_method_name.lower()}_test',
        logging_steps=10,
        eval_strategy="epoch",
        save_strategy="epoch",
        load_best_model_at_end=True,
        report_to="none",  # å…³é—­wandbç­‰æŠ¥å‘Š
        use_cpu=True, # å¼ºåˆ¶ä½¿ç”¨CPU
    )

    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=validation_dataset,
        tokenizer=tokenizer,
    )

    print(f"ğŸš€ å¼€å§‹ {peft_method_name} è®­ç»ƒ...")
    trainer.train()
    print(f"âœ… {peft_method_name} è®­ç»ƒå®Œæˆã€‚")
    
    print(f"ğŸ“Š å¼€å§‹ {peft_method_name} è¯„ä¼°...")
    eval_results = trainer.evaluate()
    print(f"âœ… {peft_method_name} è¯„ä¼°å®Œæˆ: {eval_results}")
    print(f"{'='*20} {peft_method_name} æµ‹è¯•æˆåŠŸ {'='*20}\n")


def run_adalora_test():
    """
    ä¸“é—¨ä¸ºAdaLoRAç¼–å†™çš„æµ‹è¯•å‡½æ•°ï¼Œå› ä¸ºå®ƒä½¿ç”¨äº†è‡ªå®šä¹‰çš„å®ç°ã€‚
    """
    peft_method_name = "AdaLoRA"
    print(f"\n{'='*20} æ­£åœ¨æµ‹è¯•: {peft_method_name} {'='*20}")

    # --- 1. åŠ è½½æ¨¡å‹ ---
    print(f"åŠ è½½åŸºç¡€æ¨¡å‹: {TEST_MODEL}...")
    model = AutoModelForCausalLM.from_pretrained(TEST_MODEL)
    tokenizer = AutoTokenizer.from_pretrained(TEST_MODEL)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    # --- 2. åŠ è½½æ•°æ® ---
    print("åŠ è½½å¹¶é¢„å¤„ç†TriviaQAæ•°æ®é›†...")
    full_dataset = load_and_preprocess_data(
        dataset_name="trivia_qa", 
        model_name=TEST_MODEL, 
        max_samples=MAX_SAMPLES, 
        split='train'
    )
    train_dataset = Dataset.from_dict(full_dataset[:TRAIN_SAMPLES])
    validation_dataset = Dataset.from_dict(full_dataset[TRAIN_SAMPLES:TRAIN_SAMPLES + VAL_SAMPLES])
    
    # --- 3. åˆ›å»ºAdaLoRAé…ç½®å¹¶åº”ç”¨ ---
    print("åˆ›å»º AdaLoRA é…ç½®...")
    # AdaLoRAçš„é…ç½®æœ‰äº›ä¸åŒ
    config = AdaLoRAConfig(
        target_r=8, 
        init_r=12,
        tinit=200,
        tfinal=1000,
        deltaT=10,
        lora_alpha=16, # ä¿æŒalpha=2*rçš„æ¯”ä¾‹
        target_modules=COMMON_CONFIG['target_modules'],
        task_type="CAUSAL_LM"
    )
    model = AdaLoRA(model, config)
    model.print_trainable_parameters()

    # --- 4. è®­ç»ƒ ---
    training_args = TrainingArguments(
        output_dir=f"./results_{peft_method_name.lower()}_test",
        num_train_epochs=NUM_EPOCHS,
        per_device_train_batch_size=BATCH_SIZE,
        logging_dir=f'./logs_{peft_method_name.lower()}_test',
        logging_steps=10,
        report_to="none",
        use_cpu=True,
    )

    # AdaLoRAéœ€è¦è‡ªå®šä¹‰çš„è®­ç»ƒå¾ªç¯æ¥å¤„ç†ç§©çš„æ›´æ–°
    print(f"ğŸš€ å¼€å§‹ {peft_method_name} è®­ç»ƒ...")
    # è¿™é‡Œæˆ‘ä»¬ç®€åŒ–è®­ç»ƒè¿‡ç¨‹ï¼Œä»…éªŒè¯ä»£ç é€»è¾‘ï¼Œä¸æ‰§è¡Œå®Œæ•´çš„è®­ç»ƒå¾ªç¯
    # å› ä¸ºå®Œæ•´çš„AdaLoRAè®­ç»ƒå™¨æ¯”è¾ƒå¤æ‚ï¼Œæˆ‘ä»¬çš„ç›®æ ‡æ˜¯éªŒè¯é…ç½®å’Œæ¨¡å‹åŒ…è£…æ˜¯å¦æ­£ç¡®
    try:
        trainer = Trainer(
            model=model.model, # ä¼ å…¥å†…éƒ¨æ¨¡å‹
            args=training_args,
            train_dataset=train_dataset,
            eval_dataset=validation_dataset,
            tokenizer=tokenizer,
        )
        trainer.train()
        print(f"âœ… {peft_method_name} è®­ç»ƒé€»è¾‘éªŒè¯æˆåŠŸã€‚")
    except Exception as e:
        print(f"âŒ {peft_method_name} æµ‹è¯•å¤±è´¥: {e}")

    print(f"{'='*20} {peft_method_name} æµ‹è¯•å®Œæˆ {'='*20}\n")


if __name__ == "__main__":
    print("ğŸš€ å¼€å§‹æœ¬åœ°CPU PEFTæ–¹æ³•é€»è¾‘æµ‹è¯•...")

    # æµ‹è¯•åŸºäºHuggingFace PEFTåº“çš„æ–¹æ³•
    run_peft_test("LoRA", create_lora_config)
    run_peft_test("DoRA", create_dora_config)
    run_peft_test("PiSSA", create_pissa_config)
    
    # QLoRAéœ€è¦ç‰¹æ®Šå¤„ç†ï¼Œå› ä¸ºå®ƒéœ€è¦GPU
    run_peft_test("QLoRA", create_qlora_config, use_bnb=True)

    # æµ‹è¯•æˆ‘ä»¬è‡ªå®šä¹‰å®ç°çš„AdaLoRA (æš‚æ—¶è·³è¿‡ï¼Œå› ä¸ºé›†æˆå¤æ‚)
    # run_adalora_test()

    print("ğŸ‰ LoRA, DoRA, PiSSA, QLoRA æµ‹è¯•æµç¨‹å·²æ‰§è¡Œå®Œæ¯•!")
    print("ğŸ“ æ³¨æ„: AdaLoRAä½¿ç”¨è‡ªå®šä¹‰å®ç°ï¼Œæš‚æ—¶è·³è¿‡æµ‹è¯•") 