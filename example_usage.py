# å®Œæ•´ä½¿ç”¨ç¤ºä¾‹ - åŸºäºHuggingFace PEFTçš„å„ç§å‚æ•°é«˜æ•ˆå¾®è°ƒæŠ€æœ¯
# æ¼”ç¤ºLoRAã€DoRAã€QLoRAã€PiSSAçš„ä½¿ç”¨æ–¹æ³•

import torch
import os
import sys

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append('.')

# å¯¼å…¥é…ç½®
from config import get_model, get_peft_config, MODELS

# å¯¼å…¥å„ä¸ªæŠ€æœ¯çš„æ¨¡å—
from LoRA.lora_hf import create_lora_config, load_model_with_lora
from DoRA.dora_hf import create_dora_config, load_model_with_dora  
from QLoRA.qlora_hf import create_qlora_config, load_model_with_qlora, create_bnb_config
from PiSSA.pissa_hf import create_pissa_config, load_model_with_pissa

# å¯¼å…¥AdaLoRAï¼ˆç‹¬ç«‹å®ç°ï¼‰
from AdaLoRA.adalora_config import AdaLoRAConfig

# å¯¼å…¥å…±äº«å·¥å…·
from common.data_preprocessing import create_qa_dataset, create_sample_qa_dataset
from common.training_utils import create_training_arguments, train_model, save_model_and_adapter

def demo_lora():
    """æ¼”ç¤ºLoRAä½¿ç”¨"""
    print("="*60)
    print("LoRA æ¼”ç¤º")
    print("="*60)
    
    # 1. åˆ›å»ºLoRAé…ç½®
    lora_config = create_lora_config(
        r=64,
        lora_alpha=128,  # 2 * r
        lora_dropout=0.1,
        target_modules=["q_proj", "v_proj", "k_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
    )
    
    print("LoRAé…ç½®åˆ›å»ºå®Œæˆ")
    print(f"  Rank: {lora_config.r}")
    print(f"  Alpha: {lora_config.lora_alpha}")
    print(f"  Target modules: {lora_config.target_modules}")
    
    # æ³¨æ„ï¼šå®é™…ä½¿ç”¨æ—¶æ›¿æ¢ä¸ºçœŸå®æ¨¡å‹
    # model, tokenizer = load_model_with_lora("meta-llama/Llama-3.2-3B", lora_config)
    print("\nå®é™…ä½¿ç”¨:")
    print("model, tokenizer = load_model_with_lora('meta-llama/Llama-3.2-3B', lora_config)")

def demo_dora():
    """æ¼”ç¤ºDoRAä½¿ç”¨"""
    print("="*60)
    print("DoRA æ¼”ç¤º")
    print("="*60)
    
    # 1. åˆ›å»ºDoRAé…ç½®
    dora_config = create_dora_config(
        r=64,
        lora_alpha=128,
        lora_dropout=0.1,
        use_dora=True,  # å…³é”®ï¼šå¯ç”¨DoRA
        target_modules=["q_proj", "v_proj", "k_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
    )
    
    print("DoRAé…ç½®åˆ›å»ºå®Œæˆ")
    print(f"  Rank: {dora_config.r}")
    print(f"  Alpha: {dora_config.lora_alpha}")
    print(f"  Use DoRA: {dora_config.use_dora}")
    print(f"  Target modules: {dora_config.target_modules}")
    
    print("\nå®é™…ä½¿ç”¨:")
    print("model, tokenizer = load_model_with_dora('meta-llama/Llama-3.2-3B', dora_config)")

def demo_qlora():
    """æ¼”ç¤ºQLoRAä½¿ç”¨"""
    print("="*60)
    print("QLoRA æ¼”ç¤º")
    print("="*60)
    
    # 1. åˆ›å»ºé‡åŒ–é…ç½®
    bnb_config = create_bnb_config(
        load_in_4bit=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.bfloat16,
        bnb_4bit_use_double_quant=True
    )
    
    # 2. åˆ›å»ºQLoRAé…ç½®
    qlora_config = create_qlora_config(
        r=64,
        lora_alpha=128,
        lora_dropout=0.1,
        target_modules=["q_proj", "v_proj", "k_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
    )
    
    print("QLoRAé…ç½®åˆ›å»ºå®Œæˆ")
    print(f"  4bité‡åŒ–: {bnb_config.load_in_4bit}")
    print(f"  é‡åŒ–ç±»å‹: {bnb_config.bnb_4bit_quant_type}")
    print(f"  LoRA Rank: {qlora_config.r}")
    print(f"  LoRA Alpha: {qlora_config.lora_alpha}")
    
    print("\nå®é™…ä½¿ç”¨:")
    print("model, tokenizer = load_model_with_qlora('meta-llama/Llama-3.2-3B', qlora_config, bnb_config)")

def demo_pissa():
    """æ¼”ç¤ºPiSSAä½¿ç”¨"""
    print("="*60)
    print("PiSSA æ¼”ç¤º")
    print("="*60)
    
    # 1. åˆ›å»ºPiSSAé…ç½®
    pissa_config = create_pissa_config(
        r=64,
        lora_alpha=128,
        lora_dropout=0.0,  # PiSSAæ¨èè®¾ä¸º0
        init_lora_weights="pissa_niter_4",  # å¿«é€ŸSVDåˆå§‹åŒ–
        target_modules=["q_proj", "v_proj", "k_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
    )
    
    print("PiSSAé…ç½®åˆ›å»ºå®Œæˆ")
    print(f"  Rank: {pissa_config.r}")
    print(f"  Alpha: {pissa_config.lora_alpha}")
    print(f"  Dropout: {pissa_config.lora_dropout}")
    print(f"  åˆå§‹åŒ–æ–¹æ³•: {pissa_config.init_lora_weights}")
    
    print("\nå®é™…ä½¿ç”¨:")
    print("model, tokenizer = load_model_with_pissa('meta-llama/Llama-3.2-3B', pissa_config)")

def demo_adalora():
    """æ¼”ç¤ºAdaLoRAä½¿ç”¨"""
    print("="*60)
    print("AdaLoRA æ¼”ç¤º")
    print("="*60)
    
    # AdaLoRAéœ€è¦ç‹¬ç«‹å®ç°ï¼Œä¸å®Œå…¨é›†æˆåœ¨PEFTä¸­
    adalora_config = AdaLoRAConfig(
        r=12,
        target_rank=8,
        lora_alpha=32,
        lora_dropout=0.1,
        target_modules=["q_proj", "v_proj", "k_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
        init_warmup=500,
        final_warmup=1500,
        mask_interval=10,
        beta1=0.85,
        beta2=0.85,
        orth_reg_weight=0.1
    )
    
    print("AdaLoRAé…ç½®åˆ›å»ºå®Œæˆ")
    print(f"  åˆå§‹Rank: {adalora_config.r}")
    print(f"  ç›®æ ‡Rank: {adalora_config.target_rank}")
    print(f"  Alpha: {adalora_config.lora_alpha}")
    print(f"  é¢„çƒ­æ­¥æ•°: {adalora_config.init_warmup} - {adalora_config.final_warmup}")
    
    print("\næ³¨æ„: AdaLoRAéœ€è¦ç‰¹æ®Šçš„è®­ç»ƒå¾ªç¯å’ŒRankAllocator")
    print("è¯·å‚è€ƒAdaLoRA/adalora.pyä¸­çš„å®Œæ•´å®ç°")

def demo_data_preprocessing():
    """æ¼”ç¤ºæ•°æ®é¢„å¤„ç†"""
    print("="*60)
    print("æ•°æ®é¢„å¤„ç†æ¼”ç¤º")
    print("="*60)
    
    from transformers import AutoTokenizer
    
    # æ¨¡æ‹Ÿtokenizerï¼ˆå®é™…ä½¿ç”¨æ—¶åº”è¯¥æ˜¯çœŸå®çš„tokenizerï¼‰
    print("åˆ›å»ºç¤ºä¾‹æ•°æ®é›†...")
    
    # åˆ›å»ºç¤ºä¾‹æ•°æ®é›†
    # tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-3.2-3B")
    # dataset = create_sample_qa_dataset(tokenizer, num_samples=100)
    
    print("æ•°æ®é›†åˆ›å»ºæ–¹æ³•:")
    print("1. ç¤ºä¾‹æ•°æ®: create_sample_qa_dataset(tokenizer, num_samples=100)")
    print("2. TriviaQA: create_qa_dataset('triviaqa', tokenizer, max_samples=1000)")
    print("3. NQ Open: create_qa_dataset('natural_questions', tokenizer, max_samples=1000)")

def demo_training():
    """æ¼”ç¤ºè®­ç»ƒæµç¨‹"""
    print("="*60)
    print("è®­ç»ƒæµç¨‹æ¼”ç¤º")
    print("="*60)
    
    # åˆ›å»ºè®­ç»ƒå‚æ•°
    training_args = create_training_arguments(
        output_dir="./demo_results",
        num_train_epochs=1,
        per_device_train_batch_size=2,
        gradient_accumulation_steps=4,
        learning_rate=2e-4,
        warmup_steps=50,
        logging_steps=10,
        save_steps=100
    )
    
    print("è®­ç»ƒå‚æ•°åˆ›å»ºå®Œæˆ")
    print(f"  è¾“å‡ºç›®å½•: {training_args.output_dir}")
    print(f"  è®­ç»ƒè½®æ•°: {training_args.num_train_epochs}")
    print(f"  å­¦ä¹ ç‡: {training_args.learning_rate}")
    
    print("\nå®Œæ•´è®­ç»ƒæµç¨‹:")
    print("1. é€‰æ‹©PEFTæŠ€æœ¯å¹¶åˆ›å»ºé…ç½®")
    print("2. åŠ è½½æ¨¡å‹: model, tokenizer = load_model_with_xxx(model_name, config)")
    print("3. å‡†å¤‡æ•°æ®: dataset = create_qa_dataset('triviaqa', tokenizer)")
    print("4. è®­ç»ƒæ¨¡å‹: trainer = train_model(model, tokenizer, dataset, training_args=training_args)")
    print("5. ä¿å­˜æ¨¡å‹: save_model_and_adapter(model, tokenizer, './saved_model')")

def compare_techniques():
    """å¯¹æ¯”å„ç§æŠ€æœ¯"""
    print("="*60)
    print("PEFTæŠ€æœ¯å¯¹æ¯”")
    print("="*60)
    
    comparison = """
    æŠ€æœ¯å¯¹æ¯”:
    
    1. LoRA (Low-Rank Adaptation)
       - ä¼˜åŠ¿: ç®€å•ç¨³å®šï¼Œå¹¿æ³›æ”¯æŒ
       - åŠ£åŠ¿: å›ºå®šç§©ï¼Œå‚æ•°åˆ†é…å¯èƒ½ä¸æœ€ä¼˜
       - é€‚ç”¨: å¿«é€ŸåŸå‹å’Œä¸€èˆ¬ç”¨é€”
       - æ˜¾å­˜: åŸºç¡€æ¨¡å‹å¤§å°
    
    2. DoRA (Weight-Decomposed Low-Rank Adaptation)
       - ä¼˜åŠ¿: æ€§èƒ½ä¼˜äºLoRAï¼Œç‰¹åˆ«æ˜¯ä½ç§©æ—¶
       - åŠ£åŠ¿: ç•¥å¾®å¤æ‚ï¼Œè®¡ç®—å¼€é”€ç¨å¤§
       - é€‚ç”¨: æ€§èƒ½è¦æ±‚è¾ƒé«˜çš„åœºæ™¯
       - æ˜¾å­˜: åŸºç¡€æ¨¡å‹å¤§å°
    
    3. QLoRA (4-bit Quantized LoRA)
       - ä¼˜åŠ¿: æ˜¾å­˜èŠ‚çœ65%ï¼Œæ€§èƒ½æŸå¤±æå°
       - åŠ£åŠ¿: éœ€è¦æ”¯æŒ4bité‡åŒ–çš„ç¡¬ä»¶
       - é€‚ç”¨: æ˜¾å­˜å—é™çš„ç¯å¢ƒ
       - æ˜¾å­˜: åŸºç¡€æ¨¡å‹çš„35%
    
    4. PiSSA (Principal Singular Values and Singular Vectors Adaptation)
       - ä¼˜åŠ¿: æ”¶æ•›æ›´å¿«ï¼ˆ2-5å€ï¼‰ï¼Œæ€§èƒ½æ›´å¥½
       - åŠ£åŠ¿: éœ€è¦SVDé¢„å¤„ç†ï¼Œåˆå§‹åŒ–è¾ƒæ…¢
       - é€‚ç”¨: è¿½æ±‚æœ€ä½³æ€§èƒ½å’Œæ•ˆç‡
       - æ˜¾å­˜: åŸºç¡€æ¨¡å‹å¤§å°
    
    5. AdaLoRA (Adaptive Budget Allocation)
       - ä¼˜åŠ¿: è‡ªé€‚åº”ç§©åˆ†é…ï¼Œå‚æ•°æ•ˆç‡æœ€ä¼˜
       - åŠ£åŠ¿: è®­ç»ƒå¤æ‚ï¼Œéœ€è¦è¶…å‚æ•°è°ƒä¼˜
       - é€‚ç”¨: å‚æ•°é¢„ç®—æœ‰é™ä¸”è¦æ±‚æœ€ä¼˜åˆ†é…
       - æ˜¾å­˜: åŸºç¡€æ¨¡å‹å¤§å°
    
    é€‰æ‹©å»ºè®®:
    - å¿«é€Ÿå®éªŒ: LoRA
    - æ€§èƒ½ä¼˜å…ˆ: DoRAæˆ–PiSSA
    - æ˜¾å­˜å—é™: QLoRA
    - å‚æ•°æ•ˆç‡: AdaLoRA
    - æœ€ä½³å®è·µ: PiSSA + QLoRAç»„åˆ
    """
    
    print(comparison)

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å‚æ•°é«˜æ•ˆå¾®è°ƒæŠ€æœ¯æ¼”ç¤º")
    print("åŸºäºHuggingFace PEFTåº“çš„ç»Ÿä¸€å®ç°")
    print("="*60)
    
    # æ£€æŸ¥PEFTç‰ˆæœ¬
    try:
        import peft
        print(f"PEFTç‰ˆæœ¬: {peft.__version__}")
    except ImportError:
        print("âš ï¸ PEFTåº“æœªå®‰è£…ï¼Œè¯·è¿è¡Œ: pip install peft")
        return
    
    # æ¼”ç¤ºå„ç§æŠ€æœ¯
    demo_lora()
    print()
    
    demo_dora()
    print()
    
    demo_qlora()
    print()
    
    demo_pissa()
    print()
    
    demo_adalora()
    print()
    
    demo_data_preprocessing()
    print()
    
    demo_training()
    print()
    
    compare_techniques()
    
    print("="*60)
    print("æ¼”ç¤ºå®Œæˆï¼")
    print("\nå®é™…ä½¿ç”¨æ­¥éª¤:")
    print("1. å®‰è£…ä¾èµ–: pip install -r requirements.txt")
    print("2. é€‰æ‹©æŠ€æœ¯: LoRA/DoRA/QLoRA/PiSSA/AdaLoRA")
    print("3. é…ç½®å‚æ•°: create_xxx_config(...)")
    print("4. åŠ è½½æ¨¡å‹: load_model_with_xxx(...)")
    print("5. å‡†å¤‡æ•°æ®: create_qa_dataset(...)")
    print("6. å¼€å§‹è®­ç»ƒ: train_model(...)")
    print("7. ä¿å­˜æ¨¡å‹: save_model_and_adapter(...)")

if __name__ == "__main__":
    main() 