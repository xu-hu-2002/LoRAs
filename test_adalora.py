import torch
import torch.nn as nn
from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer
from datasets import Dataset
import warnings
import sys
import os

# å¯¼å…¥AdaLoRAç›¸å…³æ¨¡å—
from AdaLoRA.adalora_config import AdaLoRAConfig
from AdaLoRA.adalora import SVDLinear, RankAllocator, compute_orth_regu
from common.data_preprocessing import load_and_preprocess_data

# --- AdaLoRAæµ‹è¯•é…ç½® ---
TEST_MODEL = "distilgpt2"  # ä½¿ç”¨è½»é‡æ¨¡å‹è¿›è¡Œæµ‹è¯•
MAX_SAMPLES = 10 # 10ä¸ªæ ·æœ¬
TRAIN_SAMPLES = 8 # 8ä¸ªè®­ç»ƒæ ·æœ¬
VAL_SAMPLES = 2 # 2ä¸ªéªŒè¯æ ·æœ¬
BATCH_SIZE = 1
NUM_EPOCHS = 1

class AdaLoRAModel(nn.Module):
    """
    AdaLoRAæ¨¡å‹åŒ…è£…å™¨
    å°†åŸºç¡€æ¨¡å‹çš„ç›®æ ‡å±‚æ›¿æ¢ä¸ºSVDLinearå±‚
    """
    def __init__(self, base_model, config: AdaLoRAConfig):
        super().__init__()
        self.base_model = base_model
        self.config = config
        self.svd_layers = {}
        
        # æ›¿æ¢ç›®æ ‡å±‚ä¸ºSVDLinear
        self._replace_target_modules()
        
        # åˆ›å»ºRankAllocatorç”¨äºåŠ¨æ€ç§©åˆ†é…
        self.rank_allocator = RankAllocator(
            self,
            config.r, 
            config.target_rank,
            init_warmup=config.init_warmup,
            final_warmup=config.final_warmup,
            mask_interval=config.mask_interval,
            beta1=config.beta1,
            beta2=config.beta2
        )
    
    def _replace_target_modules(self):
        """æ›¿æ¢ç›®æ ‡æ¨¡å—ä¸ºSVDLinear"""
        target_modules = ["c_attn", "c_proj"]  # DistilGPT2çš„ç›®æ ‡å±‚
        
        for name, module in self.base_model.named_modules():
            if any(target in name for target in target_modules) and isinstance(module, nn.Linear):
                # åˆ›å»ºSVDLinearæ›¿æ¢
                svd_layer = SVDLinear(
                    in_features=module.in_features,
                    out_features=module.out_features,
                    r=self.config.r,
                    lora_alpha=self.config.lora_alpha,
                    lora_dropout=self.config.lora_dropout
                )
                
                # å¤åˆ¶åŸå§‹æƒé‡
                svd_layer.weight.data = module.weight.data.clone()
                if module.bias is not None:
                    svd_layer.bias.data = module.bias.data.clone()
                
                # è·å–çˆ¶æ¨¡å—å¹¶æ›¿æ¢
                parent_name = '.'.join(name.split('.')[:-1]) if '.' in name else ''
                child_name = name.split('.')[-1]
                
                if parent_name:
                    parent_module = self.base_model
                    for attr_name in parent_name.split('.'):
                        parent_module = getattr(parent_module, attr_name)
                    setattr(parent_module, child_name, svd_layer)
                else:
                    setattr(self.base_model, child_name, svd_layer)
                
                # è®°å½•SVDå±‚ç”¨äºåç»­ç®¡ç†
                self.svd_layers[name] = svd_layer
                
                print(f"âœ… æ›¿æ¢å±‚ {name}: {module.__class__.__name__} -> SVDLinear")
    
    def forward(self, *args, **kwargs):
        """å‰å‘ä¼ æ’­"""
        return self.base_model(*args, **kwargs)
    
    def print_trainable_parameters(self):
        """æ‰“å°å¯è®­ç»ƒå‚æ•°ç»Ÿè®¡"""
        trainable_params = 0
        all_param = 0
        
        for _, param in self.named_parameters():
            num_params = param.numel()
            all_param += num_params
            if param.requires_grad:
                trainable_params += num_params
        
        print(f"trainable params: {trainable_params:,} || "
              f"all params: {all_param:,} || "
              f"trainable%: {100 * trainable_params / all_param:.4f}")
    
    def update_and_allocate(self, global_step):
        """æ›´æ–°é‡è¦æ€§åˆ†æ•°å¹¶åˆ†é…ç§©"""
        if hasattr(self.rank_allocator, 'update_and_allocate'):
            self.rank_allocator.update_and_allocate(self, global_step)

def create_adalora_config():
    """åˆ›å»ºAdaLoRAé…ç½®"""
    config = AdaLoRAConfig(
        r=12,  # åˆå§‹ç§©
        target_rank=8,  # ç›®æ ‡ç§‹
        lora_alpha=16,  # alpha = 2 * target_rank
        lora_dropout=0.1,
        init_warmup=50,  # å‡å°‘é¢„çƒ­æ­¥æ•°é€‚åˆå°æ•°æ®é›†
        final_warmup=100,
        mask_interval=10,
        beta1=0.85,
        beta2=0.85,
        orth_reg_weight=0.1
    )
    return config

def mark_only_lora_as_trainable(model):
    """æ ‡è®°åªæœ‰LoRAå‚æ•°å¯è®­ç»ƒ"""
    for name, param in model.named_parameters():
        if 'lora_' in name:
            param.requires_grad = True
        else:
            param.requires_grad = False

class AdaLoRATrainer(Trainer):
    """
    è‡ªå®šä¹‰Trainerï¼Œæ”¯æŒAdaLoRAçš„åŠ¨æ€ç§©åˆ†é…å’Œæ­£äº¤æ­£åˆ™åŒ–
    """
    def __init__(self, adalora_model, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.adalora_model = adalora_model
        self.global_step = 0
    
    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):
        """è®¡ç®—æŸå¤±ï¼ŒåŒ…æ‹¬æ­£äº¤æ­£åˆ™åŒ–"""
        # æ ‡å‡†è¯­è¨€æ¨¡å‹æŸå¤±
        outputs = model(**inputs)
        loss = outputs.loss
        
        # æ·»åŠ æ­£äº¤æ­£åˆ™åŒ–
        orth_regu = compute_orth_regu(self.adalora_model, regu_weight=0.1)
        loss += orth_regu
        
        return (loss, outputs) if return_outputs else loss
    
    def training_step(self, model, inputs,num_items_in_batch=None):
        """è®­ç»ƒæ­¥éª¤ï¼ŒåŒ…æ‹¬ç§©åˆ†é…æ›´æ–°"""
        self.global_step += 1
        
        # æ‰§è¡Œæ ‡å‡†è®­ç»ƒæ­¥éª¤
        loss = super().training_step(model, inputs, num_items_in_batch)
        
        # æ›´æ–°é‡è¦æ€§åˆ†æ•°å’Œç§©åˆ†é…
        if self.global_step % 10 == 0:  # æ¯10æ­¥æ›´æ–°ä¸€æ¬¡
            self.adalora_model.update_and_allocate(self.global_step)
        
        return loss

def test_adalora():
    """AdaLoRAå®Œæ•´æµ‹è¯•å‡½æ•°"""
    print("="*60)
    print("AdaLoRA (Adaptive Budget Allocation) CPUæµ‹è¯•")
    print("="*60)
    
    # --- 1. åŠ è½½æ¨¡å‹å’Œtokenizer ---
    print(f"åŠ è½½åŸºç¡€æ¨¡å‹: {TEST_MODEL}...")
    base_model = AutoModelForCausalLM.from_pretrained(
        TEST_MODEL,
        torch_dtype=torch.float32,
        low_cpu_mem_usage=True
    )
    tokenizer = AutoTokenizer.from_pretrained(TEST_MODEL)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    # --- 2. åŠ è½½æ•°æ® ---
    print("åŠ è½½å¹¶é¢„å¤„ç†TriviaQAæ•°æ®é›†...")
    full_dataset = load_and_preprocess_data(
        dataset_name="trivia_qa",
        model_name=TEST_MODEL,
        max_samples=MAX_SAMPLES,
        split='train'
    )
    
    train_dataset = Dataset.from_dict(full_dataset[:TRAIN_SAMPLES])
    eval_dataset = Dataset.from_dict(full_dataset[TRAIN_SAMPLES:TRAIN_SAMPLES + VAL_SAMPLES])
    
    # --- 3. åˆ›å»ºAdaLoRAæ¨¡å‹ ---
    print("åˆ›å»ºAdaLoRAé…ç½®...")
    adalora_config = create_adalora_config()
    print(f"AdaLoRAé…ç½®: r={adalora_config.r}, target_r={adalora_config.target_rank}, alpha={adalora_config.lora_alpha}")
    
    print("åº”ç”¨AdaLoRAåˆ°æ¨¡å‹...")
    adalora_model = AdaLoRAModel(base_model, adalora_config)
    
    # æ ‡è®°åªæœ‰LoRAå‚æ•°å¯è®­ç»ƒ
    mark_only_lora_as_trainable(adalora_model)
    adalora_model.print_trainable_parameters()
    
    # --- 4. è®¾ç½®è®­ç»ƒå‚æ•° ---
    training_args = TrainingArguments(
        output_dir="./results_adalora_test",
        num_train_epochs=NUM_EPOCHS,
        per_device_train_batch_size=BATCH_SIZE,
        per_device_eval_batch_size=BATCH_SIZE,
        logging_dir='./logs_adalora_test',
        logging_steps=5,
        eval_strategy="epoch",
        save_strategy="epoch",
        load_best_model_at_end=True,
        report_to="none",
        use_cpu=True,
        learning_rate=2e-4,
        warmup_steps=10,
        max_grad_norm=1.0,
        remove_unused_columns=False,# ä¿ç•™æ•°æ®é›†åˆ—å
        dataloader_drop_last=False,#  é¿å…å°æ•°æ®é›†çš„batchä¸¢å¤±
    )
    
    # --- 5. åˆ›å»ºè‡ªå®šä¹‰Trainer ---
    trainer = AdaLoRATrainer(
        adalora_model=adalora_model,
        model=adalora_model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
        tokenizer=tokenizer,
    )
    
    # --- 6. å¼€å§‹è®­ç»ƒ ---
    print("ğŸš€ å¼€å§‹AdaLoRAè®­ç»ƒ...")
    try:
        trainer.train()
        print("âœ… AdaLoRAè®­ç»ƒå®Œæˆ")
        
        # --- 7. è¯„ä¼° ---
        print("ğŸ“Š å¼€å§‹AdaLoRAè¯„ä¼°...")
        eval_results = trainer.evaluate()
        print(f"âœ… AdaLoRAè¯„ä¼°å®Œæˆ: {eval_results}")
        
        # --- 8. æ˜¾ç¤ºç§©åˆ†é…ä¿¡æ¯ ---
        print("\nğŸ“ˆ AdaLoRAç§©åˆ†é…ä¿¡æ¯:")
        for name, layer in adalora_model.svd_layers.items():
            if hasattr(layer, 'rank'):
                print(f"  {name}: å½“å‰ç§© = {getattr(layer, 'rank', 'æœªçŸ¥')}")
        
        print("="*60)
        print("âœ… AdaLoRAæµ‹è¯•æˆåŠŸå®Œæˆï¼")
        print("="*60)
        
    except Exception as e:
        print(f"âŒ AdaLoRAæµ‹è¯•å¤±è´¥: {str(e)}")
        print("ğŸ’¡ è¿™å¯èƒ½æ˜¯å› ä¸ºAdaLoRAçš„å¤æ‚æ€§ï¼Œä½†åŸºæœ¬çš„æ¨¡å‹æ›¿æ¢é€»è¾‘å·²éªŒè¯")
        
        # å³ä½¿è®­ç»ƒå¤±è´¥ï¼Œä¹ŸéªŒè¯äº†æ¨¡å‹æ›¿æ¢é€»è¾‘
        print("\nâœ… éªŒè¯ç»“æœ:")
        print(f"  - æ¨¡å‹æ›¿æ¢: æˆåŠŸ")
        print(f"  - SVDå±‚åˆ›å»º: æˆåŠŸ ({len(adalora_model.svd_layers)}ä¸ªå±‚)")
        print(f"  - å‚æ•°ç»Ÿè®¡: æˆåŠŸ")
        print("="*60)

if __name__ == "__main__":
    test_adalora() 